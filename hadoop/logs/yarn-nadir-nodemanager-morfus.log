2015-07-29 10:14:40,335 INFO org.apache.hadoop.yarn.server.nodemanager.NodeManager: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NodeManager
STARTUP_MSG:   host = morfus/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/etc/hadoop/nm-config/log4j.properties
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-07-29 10:14:40,449 INFO org.apache.hadoop.yarn.server.nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]
2015-07-29 10:14:48,233 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher
2015-07-29 10:14:48,243 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher
2015-07-29 10:14:48,244 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService
2015-07-29 10:14:48,245 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices
2015-07-29 10:14:48,264 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl
2015-07-29 10:14:48,267 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher
2015-07-29 10:14:48,382 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl
2015-07-29 10:14:48,386 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager
2015-07-29 10:14:48,537 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-07-29 10:14:48,853 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-07-29 10:14:48,853 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NodeManager metrics system started
2015-07-29 10:14:49,259 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler
2015-07-29 10:14:49,275 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService
2015-07-29 10:14:49,275 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: per directory file limit = 8192
2015-07-29 10:14:49,434 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker
2015-07-29 10:14:49,525 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: The Auxilurary Service named 'mapreduce_shuffle' in the configuration is for class org.apache.hadoop.mapred.ShuffleHandler which has a name of 'httpshuffle'. Because these are not the same tools trying to send ServiceData and read Service Meta Data may have issues unless the refer to the name in the config.
2015-07-29 10:14:49,525 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Adding auxiliary service httpshuffle, "mapreduce_shuffle"
2015-07-29 10:14:49,728 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin@401623f
2015-07-29 10:14:49,729 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null
2015-07-29 10:14:49,730 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Physical memory check enabled: true
2015-07-29 10:14:49,730 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Virtual memory check enabled: true
2015-07-29 10:14:49,766 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: NodeManager configured with 8 G physical memory allocated to containers, which is more than 80% of the total physical memory available (5.7 G). Thrashing might happen.
2015-07-29 10:14:49,791 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager for null: physical-memory=8192 virtual-memory=17204 virtual-cores=8
2015-07-29 10:14:49,939 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-07-29 10:14:50,034 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 60035
2015-07-29 10:14:50,303 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server
2015-07-29 10:14:50,303 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Blocking new container-requests as container manager rpc server is still starting.
2015-07-29 10:14:50,304 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-07-29 10:14:50,342 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 60035: starting
2015-07-29 10:14:50,358 INFO org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager: Updating node address : morfus:60035
2015-07-29 10:14:50,400 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-07-29 10:14:50,401 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8040
2015-07-29 10:14:50,413 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server
2015-07-29 10:14:50,417 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-07-29 10:14:50,427 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Localizer started on port 8040
2015-07-29 10:14:50,431 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8040: starting
2015-07-29 10:14:50,475 INFO org.apache.hadoop.mapred.IndexCache: IndexCache created with max memory = 10485760
2015-07-29 10:14:50,534 INFO org.apache.hadoop.mapred.ShuffleHandler: httpshuffle listening on port 13562
2015-07-29 10:14:50,540 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: ContainerManager started at morfus/127.0.1.1:60035
2015-07-29 10:14:50,540 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: ContainerManager bound to 0.0.0.0/0.0.0.0:0
2015-07-29 10:14:50,548 INFO org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer: Instantiating NMWebApp at 0.0.0.0:8042
2015-07-29 10:14:50,982 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-07-29 10:14:51,059 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-07-29 10:14:51,082 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined
2015-07-29 10:14:51,106 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-07-29 10:14:51,114 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node
2015-07-29 10:14:51,114 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-07-29 10:14:51,114 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-07-29 10:14:51,152 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /node/*
2015-07-29 10:14:51,153 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /ws/*
2015-07-29 10:14:51,200 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 8042
2015-07-29 10:14:51,200 INFO org.mortbay.log: jetty-6.1.26
2015-07-29 10:14:51,361 INFO org.mortbay.log: Extract jar:file:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar!/webapps/node to /tmp/Jetty_0_0_0_0_8042_node____19tj0x/webapp
2015-07-29 10:14:52,958 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8042
2015-07-29 10:14:52,958 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app /node started at 8042
2015-07-29 10:14:55,578 INFO org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules
2015-07-29 10:14:55,723 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8031
2015-07-29 10:14:55,934 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []
2015-07-29 10:14:55,982 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]
2015-07-29 10:14:57,256 INFO org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -715654368
2015-07-29 10:14:57,299 INFO org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id -397618082
2015-07-29 10:14:57,300 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as morfus:60035 with total resource of <memory:8192, vCores:8>
2015-07-29 10:14:57,300 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Notifying ContainerManager to unblock new container-requests
2015-07-29 10:40:48,483 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0001_000001 (auth:SIMPLE)
2015-07-29 10:40:48,714 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1438157687835_0001_01_000001 by user nadir
2015-07-29 10:40:48,775 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Creating a new application reference for app application_1438157687835_0001
2015-07-29 10:40:48,789 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0001 transitioned from NEW to INITING
2015-07-29 10:40:48,809 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1438157687835_0001_01_000001 to application application_1438157687835_0001
2015-07-29 10:40:48,812 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0001	CONTAINERID=container_1438157687835_0001_01_000001
2015-07-29 10:40:48,820 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0001 transitioned from INITING to RUNNING
2015-07-29 10:40:48,848 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0001_01_000001 transitioned from NEW to LOCALIZING
2015-07-29 10:40:48,848 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1438157687835_0001
2015-07-29 10:40:48,899 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp287533196/tmp280343713/joda-time-2.1.jar transitioned from INIT to DOWNLOADING
2015-07-29 10:40:48,899 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp287533196/tmp-1897459595/guava-11.0.2.jar transitioned from INIT to DOWNLOADING
2015-07-29 10:40:48,899 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp287533196/tmp-1316592451/antlr-runtime-3.4.jar transitioned from INIT to DOWNLOADING
2015-07-29 10:40:48,899 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp287533196/tmp248187391/pig-0.14.0-core-h2.jar transitioned from INIT to DOWNLOADING
2015-07-29 10:40:48,899 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp287533196/tmp1197236651/automaton-1.11-8.jar transitioned from INIT to DOWNLOADING
2015-07-29 10:40:48,900 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0001/job.splitmetainfo transitioned from INIT to DOWNLOADING
2015-07-29 10:40:48,900 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0001/job.split transitioned from INIT to DOWNLOADING
2015-07-29 10:40:48,900 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0001/job.xml transitioned from INIT to DOWNLOADING
2015-07-29 10:40:48,900 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Created localizer for container_1438157687835_0001_01_000001
2015-07-29 10:40:49,104 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-nadir/nm-local-dir/nmPrivate/container_1438157687835_0001_01_000001.tokens. Credentials list: 
2015-07-29 10:40:49,154 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Initializing user nadir
2015-07-29 10:40:49,285 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-nadir/nm-local-dir/nmPrivate/container_1438157687835_0001_01_000001.tokens to /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0001/container_1438157687835_0001_01_000001.tokens
2015-07-29 10:40:49,288 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0001 = file:/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0001
2015-07-29 10:40:51,349 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp287533196/tmp280343713/joda-time-2.1.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/10/joda-time-2.1.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:40:51,444 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp287533196/tmp-1897459595/guava-11.0.2.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/11/guava-11.0.2.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:40:51,503 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp287533196/tmp-1316592451/antlr-runtime-3.4.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/12/antlr-runtime-3.4.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:40:51,690 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp287533196/tmp248187391/pig-0.14.0-core-h2.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/13/pig-0.14.0-core-h2.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:40:51,765 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp287533196/tmp1197236651/automaton-1.11-8.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/14/automaton-1.11-8.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:40:51,798 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0001/job.splitmetainfo(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0001/filecache/10/job.splitmetainfo) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:40:51,832 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0001/job.split(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0001/filecache/11/job.split) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:40:51,872 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0001/job.xml(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0001/filecache/12/job.xml) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:40:51,876 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0001_01_000001 transitioned from LOCALIZING to LOCALIZED
2015-07-29 10:40:51,973 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0001_01_000001 transitioned from LOCALIZED to RUNNING
2015-07-29 10:40:51,986 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0001/container_1438157687835_0001_01_000001/default_container_executor.sh]
2015-07-29 10:40:53,614 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1438157687835_0001_01_000001
2015-07-29 10:40:53,895 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18853 for container-id container_1438157687835_0001_01_000001: 79.1 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 10:40:57,130 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18853 for container-id container_1438157687835_0001_01_000001: 107.9 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 10:41:00,220 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18853 for container-id container_1438157687835_0001_01_000001: 120.1 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 10:41:03,396 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18853 for container-id container_1438157687835_0001_01_000001: 132.4 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:41:06,502 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18853 for container-id container_1438157687835_0001_01_000001: 140.8 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:41:09,621 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18853 for container-id container_1438157687835_0001_01_000001: 148.8 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:41:09,961 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0001_000001 (auth:SIMPLE)
2015-07-29 10:41:09,970 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1438157687835_0001_01_000002 by user nadir
2015-07-29 10:41:09,971 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1438157687835_0001_01_000002 to application application_1438157687835_0001
2015-07-29 10:41:09,972 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0001_01_000002 transitioned from NEW to LOCALIZING
2015-07-29 10:41:09,972 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1438157687835_0001
2015-07-29 10:41:09,972 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_INIT for appId application_1438157687835_0001
2015-07-29 10:41:09,972 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got APPLICATION_INIT for service mapreduce_shuffle
2015-07-29 10:41:09,976 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0001	CONTAINERID=container_1438157687835_0001_01_000002
2015-07-29 10:41:09,988 INFO org.apache.hadoop.mapred.ShuffleHandler: Added token for job_1438157687835_0001
2015-07-29 10:41:09,989 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0001_01_000002 transitioned from LOCALIZING to LOCALIZED
2015-07-29 10:41:10,050 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0001_01_000002 transitioned from LOCALIZED to RUNNING
2015-07-29 10:41:10,058 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0001/container_1438157687835_0001_01_000002/default_container_executor.sh]
2015-07-29 10:41:12,621 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1438157687835_0001_01_000002
2015-07-29 10:41:12,792 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18853 for container-id container_1438157687835_0001_01_000001: 150.1 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:41:12,942 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18952 for container-id container_1438157687835_0001_01_000002: 86.0 MB of 1 GB physical memory used; 724.4 MB of 2.1 GB virtual memory used
2015-07-29 10:41:16,021 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18853 for container-id container_1438157687835_0001_01_000001: 150.1 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:41:16,151 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18952 for container-id container_1438157687835_0001_01_000002: 111.5 MB of 1 GB physical memory used; 757.6 MB of 2.1 GB virtual memory used
2015-07-29 10:41:19,238 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18853 for container-id container_1438157687835_0001_01_000001: 150.1 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:41:19,323 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18952 for container-id container_1438157687835_0001_01_000002: 133.4 MB of 1 GB physical memory used; 758.7 MB of 2.1 GB virtual memory used
2015-07-29 10:41:22,429 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18853 for container-id container_1438157687835_0001_01_000001: 150.1 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:41:22,572 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18952 for container-id container_1438157687835_0001_01_000002: 244.9 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 10:41:23,940 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0001_000001 (auth:SIMPLE)
2015-07-29 10:41:23,958 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Stopping container with container Id: container_1438157687835_0001_01_000002
2015-07-29 10:41:23,961 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0001_01_000002 transitioned from RUNNING to KILLING
2015-07-29 10:41:23,961 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1438157687835_0001_01_000002
2015-07-29 10:41:23,963 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0001	CONTAINERID=container_1438157687835_0001_01_000002
2015-07-29 10:41:24,001 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1438157687835_0001_01_000002 is : 143
2015-07-29 10:41:24,095 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0001_01_000002 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
2015-07-29 10:41:24,108 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0001/container_1438157687835_0001_01_000002
2015-07-29 10:41:24,121 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1438157687835_0001	CONTAINERID=container_1438157687835_0001_01_000002
2015-07-29 10:41:24,138 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0001_01_000002 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
2015-07-29 10:41:24,138 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1438157687835_0001_01_000002 from application application_1438157687835_0001
2015-07-29 10:41:24,138 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1438157687835_0001
2015-07-29 10:41:25,572 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1438157687835_0001_01_000002
2015-07-29 10:41:25,614 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18853 for container-id container_1438157687835_0001_01_000001: 150.2 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:41:26,717 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0001_000001 (auth:SIMPLE)
2015-07-29 10:41:26,738 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1438157687835_0001_01_000003 by user nadir
2015-07-29 10:41:26,739 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1438157687835_0001_01_000003 to application application_1438157687835_0001
2015-07-29 10:41:26,739 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0001_01_000003 transitioned from NEW to LOCALIZING
2015-07-29 10:41:26,739 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1438157687835_0001
2015-07-29 10:41:26,740 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_INIT for appId application_1438157687835_0001
2015-07-29 10:41:26,740 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got APPLICATION_INIT for service mapreduce_shuffle
2015-07-29 10:41:26,740 INFO org.apache.hadoop.mapred.ShuffleHandler: Added token for job_1438157687835_0001
2015-07-29 10:41:26,743 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0001_01_000003 transitioned from LOCALIZING to LOCALIZED
2015-07-29 10:41:26,743 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0001	CONTAINERID=container_1438157687835_0001_01_000003
2015-07-29 10:41:26,804 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0001_01_000003 transitioned from LOCALIZED to RUNNING
2015-07-29 10:41:26,813 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0001/container_1438157687835_0001_01_000003/default_container_executor.sh]
2015-07-29 10:41:26,979 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1438157687835_0001_01_000002]
2015-07-29 10:41:28,614 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1438157687835_0001_01_000003
2015-07-29 10:41:28,771 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18853 for container-id container_1438157687835_0001_01_000001: 150.2 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:41:28,902 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19013 for container-id container_1438157687835_0001_01_000003: 80.0 MB of 1 GB physical memory used; 724.6 MB of 2.1 GB virtual memory used
2015-07-29 10:41:32,039 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18853 for container-id container_1438157687835_0001_01_000001: 150.2 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:41:32,127 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19013 for container-id container_1438157687835_0001_01_000003: 105.3 MB of 1 GB physical memory used; 732.2 MB of 2.1 GB virtual memory used
2015-07-29 10:41:35,224 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18853 for container-id container_1438157687835_0001_01_000001: 150.2 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:41:35,309 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19013 for container-id container_1438157687835_0001_01_000003: 130.4 MB of 1 GB physical memory used; 760.0 MB of 2.1 GB virtual memory used
2015-07-29 10:41:38,256 INFO org.apache.hadoop.mapred.ShuffleHandler: Setting connection close header...
2015-07-29 10:41:38,531 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18853 for container-id container_1438157687835_0001_01_000001: 150.2 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:41:38,713 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19013 for container-id container_1438157687835_0001_01_000003: 138.5 MB of 1 GB physical memory used; 770.0 MB of 2.1 GB virtual memory used
2015-07-29 10:41:40,879 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0001_000001 (auth:SIMPLE)
2015-07-29 10:41:40,888 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Stopping container with container Id: container_1438157687835_0001_01_000003
2015-07-29 10:41:40,888 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0001	CONTAINERID=container_1438157687835_0001_01_000003
2015-07-29 10:41:40,889 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0001_01_000003 transitioned from RUNNING to KILLING
2015-07-29 10:41:40,889 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1438157687835_0001_01_000003
2015-07-29 10:41:40,956 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1438157687835_0001_01_000003 is : 143
2015-07-29 10:41:41,099 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0001_01_000003 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
2015-07-29 10:41:41,113 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0001/container_1438157687835_0001_01_000003
2015-07-29 10:41:41,123 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1438157687835_0001	CONTAINERID=container_1438157687835_0001_01_000003
2015-07-29 10:41:41,131 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0001_01_000003 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
2015-07-29 10:41:41,131 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1438157687835_0001_01_000003 from application application_1438157687835_0001
2015-07-29 10:41:41,131 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1438157687835_0001
2015-07-29 10:41:41,726 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1438157687835_0001_01_000003
2015-07-29 10:41:41,775 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18853 for container-id container_1438157687835_0001_01_000001: 150.8 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:41:44,840 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 18853 for container-id container_1438157687835_0001_01_000001: 151.5 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:41:47,696 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Container container_1438157687835_0001_01_000001 succeeded 
2015-07-29 10:41:47,696 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0001_01_000001 transitioned from RUNNING to EXITED_WITH_SUCCESS
2015-07-29 10:41:47,697 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1438157687835_0001_01_000001
2015-07-29 10:41:47,737 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0001/container_1438157687835_0001_01_000001
2015-07-29 10:41:47,740 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1438157687835_0001	CONTAINERID=container_1438157687835_0001_01_000001
2015-07-29 10:41:47,740 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0001_01_000001 transitioned from EXITED_WITH_SUCCESS to DONE
2015-07-29 10:41:47,740 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1438157687835_0001_01_000001 from application application_1438157687835_0001
2015-07-29 10:41:47,741 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1438157687835_0001
2015-07-29 10:41:47,841 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1438157687835_0001_01_000001
2015-07-29 10:41:47,958 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0001_000001 (auth:SIMPLE)
2015-07-29 10:41:47,969 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Stopping container with container Id: container_1438157687835_0001_01_000001
2015-07-29 10:41:47,972 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0001	CONTAINERID=container_1438157687835_0001_01_000001
2015-07-29 10:41:47,980 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1438157687835_0001_01_000001]
2015-07-29 10:41:47,990 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0001 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP
2015-07-29 10:41:47,991 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0001
2015-07-29 10:41:47,993 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_STOP for appId application_1438157687835_0001
2015-07-29 10:41:47,996 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0001 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED
2015-07-29 10:41:47,997 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler: Scheduling Log Deletion for application: application_1438157687835_0001, with delay of 10800 seconds
2015-07-29 10:50:29,416 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0003_000001 (auth:SIMPLE)
2015-07-29 10:50:29,426 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1438157687835_0003_01_000001 by user nadir
2015-07-29 10:50:29,427 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Creating a new application reference for app application_1438157687835_0003
2015-07-29 10:50:29,427 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0003 transitioned from NEW to INITING
2015-07-29 10:50:29,428 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0003 transitioned from INITING to RUNNING
2015-07-29 10:50:29,428 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1438157687835_0003_01_000001 to application application_1438157687835_0003
2015-07-29 10:50:29,429 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0003_01_000001 transitioned from NEW to LOCALIZING
2015-07-29 10:50:29,429 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1438157687835_0003
2015-07-29 10:50:29,429 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1874738908/tmp-1473912263/joda-time-2.1.jar transitioned from INIT to DOWNLOADING
2015-07-29 10:50:29,429 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1874738908/tmp939540298/guava-11.0.2.jar transitioned from INIT to DOWNLOADING
2015-07-29 10:50:29,429 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1874738908/tmp-362406365/antlr-runtime-3.4.jar transitioned from INIT to DOWNLOADING
2015-07-29 10:50:29,430 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1874738908/tmp-1159510277/pig-0.14.0-core-h2.jar transitioned from INIT to DOWNLOADING
2015-07-29 10:50:29,430 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1874738908/tmp1351741301/automaton-1.11-8.jar transitioned from INIT to DOWNLOADING
2015-07-29 10:50:29,430 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0003/job.splitmetainfo transitioned from INIT to DOWNLOADING
2015-07-29 10:50:29,430 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0003/job.split transitioned from INIT to DOWNLOADING
2015-07-29 10:50:29,430 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0003/job.xml transitioned from INIT to DOWNLOADING
2015-07-29 10:50:29,430 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Created localizer for container_1438157687835_0003_01_000001
2015-07-29 10:50:29,430 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0003	CONTAINERID=container_1438157687835_0003_01_000001
2015-07-29 10:50:29,436 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-nadir/nm-local-dir/nmPrivate/container_1438157687835_0003_01_000001.tokens. Credentials list: 
2015-07-29 10:50:29,450 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Initializing user nadir
2015-07-29 10:50:29,453 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-nadir/nm-local-dir/nmPrivate/container_1438157687835_0003_01_000001.tokens to /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0003/container_1438157687835_0003_01_000001.tokens
2015-07-29 10:50:29,454 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0003 = file:/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0003
2015-07-29 10:50:29,541 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1874738908/tmp-1473912263/joda-time-2.1.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/15/joda-time-2.1.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:50:29,682 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1874738908/tmp939540298/guava-11.0.2.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/16/guava-11.0.2.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:50:29,715 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1874738908/tmp-362406365/antlr-runtime-3.4.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/17/antlr-runtime-3.4.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:50:29,813 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1874738908/tmp-1159510277/pig-0.14.0-core-h2.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/18/pig-0.14.0-core-h2.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:50:29,847 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1874738908/tmp1351741301/automaton-1.11-8.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/19/automaton-1.11-8.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:50:29,874 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0003/job.splitmetainfo(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0003/filecache/10/job.splitmetainfo) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:50:29,904 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0003/job.split(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0003/filecache/11/job.split) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:50:29,966 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0003/job.xml(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0003/filecache/12/job.xml) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:50:29,967 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0003_01_000001 transitioned from LOCALIZING to LOCALIZED
2015-07-29 10:50:30,025 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0003_01_000001 transitioned from LOCALIZED to RUNNING
2015-07-29 10:50:30,033 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0003/container_1438157687835_0003_01_000001/default_container_executor.sh]
2015-07-29 10:50:32,861 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1438157687835_0003_01_000001
2015-07-29 10:50:33,033 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19662 for container-id container_1438157687835_0003_01_000001: 87.2 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 10:50:36,110 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19662 for container-id container_1438157687835_0003_01_000001: 109.8 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 10:50:39,247 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19662 for container-id container_1438157687835_0003_01_000001: 119.1 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 10:50:42,309 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19662 for container-id container_1438157687835_0003_01_000001: 128.5 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 10:50:45,372 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19662 for container-id container_1438157687835_0003_01_000001: 139.7 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:50:48,232 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0003_000001 (auth:SIMPLE)
2015-07-29 10:50:48,240 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1438157687835_0003_01_000002 by user nadir
2015-07-29 10:50:48,241 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1438157687835_0003_01_000002 to application application_1438157687835_0003
2015-07-29 10:50:48,242 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0003_01_000002 transitioned from NEW to LOCALIZING
2015-07-29 10:50:48,242 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1438157687835_0003
2015-07-29 10:50:48,242 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_INIT for appId application_1438157687835_0003
2015-07-29 10:50:48,242 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got APPLICATION_INIT for service mapreduce_shuffle
2015-07-29 10:50:48,242 INFO org.apache.hadoop.mapred.ShuffleHandler: Added token for job_1438157687835_0003
2015-07-29 10:50:48,244 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0003_01_000002 transitioned from LOCALIZING to LOCALIZED
2015-07-29 10:50:48,244 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0003	CONTAINERID=container_1438157687835_0003_01_000002
2015-07-29 10:50:48,323 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0003_01_000002 transitioned from LOCALIZED to RUNNING
2015-07-29 10:50:48,331 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0003/container_1438157687835_0003_01_000002/default_container_executor.sh]
2015-07-29 10:50:48,372 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1438157687835_0003_01_000002
2015-07-29 10:50:48,442 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19662 for container-id container_1438157687835_0003_01_000001: 146.6 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:50:48,538 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19762 for container-id container_1438157687835_0003_01_000002: 24.9 MB of 1 GB physical memory used; 689.4 MB of 2.1 GB virtual memory used
2015-07-29 10:50:51,648 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19662 for container-id container_1438157687835_0003_01_000001: 146.6 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:50:51,708 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19762 for container-id container_1438157687835_0003_01_000002: 92.2 MB of 1 GB physical memory used; 728.0 MB of 2.1 GB virtual memory used
2015-07-29 10:50:54,764 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19662 for container-id container_1438157687835_0003_01_000001: 146.8 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:50:54,830 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19762 for container-id container_1438157687835_0003_01_000002: 119.2 MB of 1 GB physical memory used; 757.9 MB of 2.1 GB virtual memory used
2015-07-29 10:50:57,889 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19662 for container-id container_1438157687835_0003_01_000001: 146.8 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:50:57,976 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19762 for container-id container_1438157687835_0003_01_000002: 131.1 MB of 1 GB physical memory used; 761.0 MB of 2.1 GB virtual memory used
2015-07-29 10:51:00,975 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0003_000001 (auth:SIMPLE)
2015-07-29 10:51:00,992 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Stopping container with container Id: container_1438157687835_0003_01_000002
2015-07-29 10:51:00,993 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0003_01_000002 transitioned from RUNNING to KILLING
2015-07-29 10:51:00,993 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1438157687835_0003_01_000002
2015-07-29 10:51:00,993 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0003	CONTAINERID=container_1438157687835_0003_01_000002
2015-07-29 10:51:01,086 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1438157687835_0003_01_000002 is : 143
2015-07-29 10:51:01,235 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19662 for container-id container_1438157687835_0003_01_000001: 147.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:51:01,260 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0003_01_000002 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
2015-07-29 10:51:01,274 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0003/container_1438157687835_0003_01_000002
2015-07-29 10:51:01,282 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1438157687835_0003	CONTAINERID=container_1438157687835_0003_01_000002
2015-07-29 10:51:01,283 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0003_01_000002 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
2015-07-29 10:51:01,283 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1438157687835_0003_01_000002 from application application_1438157687835_0003
2015-07-29 10:51:01,283 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1438157687835_0003
2015-07-29 10:51:01,334 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19762 for container-id container_1438157687835_0003_01_000002: -1B of 1 GB physical memory used; -1B of 2.1 GB virtual memory used
2015-07-29 10:51:04,334 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1438157687835_0003_01_000002
2015-07-29 10:51:04,362 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19662 for container-id container_1438157687835_0003_01_000001: 148.5 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:51:07,535 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 19662 for container-id container_1438157687835_0003_01_000001: 148.5 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:51:07,840 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Container container_1438157687835_0003_01_000001 succeeded 
2015-07-29 10:51:07,841 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0003_01_000001 transitioned from RUNNING to EXITED_WITH_SUCCESS
2015-07-29 10:51:07,841 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1438157687835_0003_01_000001
2015-07-29 10:51:07,940 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0003/container_1438157687835_0003_01_000001
2015-07-29 10:51:07,954 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1438157687835_0003	CONTAINERID=container_1438157687835_0003_01_000001
2015-07-29 10:51:07,954 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0003_01_000001 transitioned from EXITED_WITH_SUCCESS to DONE
2015-07-29 10:51:07,954 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1438157687835_0003_01_000001 from application application_1438157687835_0003
2015-07-29 10:51:07,954 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1438157687835_0003
2015-07-29 10:51:08,057 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0003_000001 (auth:SIMPLE)
2015-07-29 10:51:08,068 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Stopping container with container Id: container_1438157687835_0003_01_000001
2015-07-29 10:51:08,068 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0003	CONTAINERID=container_1438157687835_0003_01_000001
2015-07-29 10:51:08,072 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1438157687835_0003_01_000001]
2015-07-29 10:51:08,077 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0003 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP
2015-07-29 10:51:08,078 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0003
2015-07-29 10:51:08,086 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_STOP for appId application_1438157687835_0003
2015-07-29 10:51:08,086 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0003 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED
2015-07-29 10:51:08,086 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler: Scheduling Log Deletion for application: application_1438157687835_0003, with delay of 10800 seconds
2015-07-29 10:51:10,536 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1438157687835_0003_01_000001
2015-07-29 10:55:03,718 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0004_000001 (auth:SIMPLE)
2015-07-29 10:55:03,731 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1438157687835_0004_01_000001 by user nadir
2015-07-29 10:55:03,732 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Creating a new application reference for app application_1438157687835_0004
2015-07-29 10:55:03,734 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0004 transitioned from NEW to INITING
2015-07-29 10:55:03,734 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0004 transitioned from INITING to RUNNING
2015-07-29 10:55:03,734 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1438157687835_0004_01_000001 to application application_1438157687835_0004
2015-07-29 10:55:03,735 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0004_01_000001 transitioned from NEW to LOCALIZING
2015-07-29 10:55:03,735 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1438157687835_0004
2015-07-29 10:55:03,735 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-794010729/tmp226081973/joda-time-2.1.jar transitioned from INIT to DOWNLOADING
2015-07-29 10:55:03,735 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-794010729/tmp-1972115276/guava-11.0.2.jar transitioned from INIT to DOWNLOADING
2015-07-29 10:55:03,738 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-794010729/tmp2110365162/antlr-runtime-3.4.jar transitioned from INIT to DOWNLOADING
2015-07-29 10:55:03,738 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-794010729/tmp-268150203/pig-0.14.0-core-h2.jar transitioned from INIT to DOWNLOADING
2015-07-29 10:55:03,738 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-794010729/tmp2039871689/automaton-1.11-8.jar transitioned from INIT to DOWNLOADING
2015-07-29 10:55:03,740 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0004/job.splitmetainfo transitioned from INIT to DOWNLOADING
2015-07-29 10:55:03,741 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0004/job.split transitioned from INIT to DOWNLOADING
2015-07-29 10:55:03,741 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0004/job.xml transitioned from INIT to DOWNLOADING
2015-07-29 10:55:03,741 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Created localizer for container_1438157687835_0004_01_000001
2015-07-29 10:55:03,741 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0004	CONTAINERID=container_1438157687835_0004_01_000001
2015-07-29 10:55:03,752 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-nadir/nm-local-dir/nmPrivate/container_1438157687835_0004_01_000001.tokens. Credentials list: 
2015-07-29 10:55:03,783 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Initializing user nadir
2015-07-29 10:55:03,790 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-nadir/nm-local-dir/nmPrivate/container_1438157687835_0004_01_000001.tokens to /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0004/container_1438157687835_0004_01_000001.tokens
2015-07-29 10:55:03,791 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0004 = file:/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0004
2015-07-29 10:55:03,929 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-794010729/tmp226081973/joda-time-2.1.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/20/joda-time-2.1.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:55:03,981 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-794010729/tmp-1972115276/guava-11.0.2.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/21/guava-11.0.2.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:55:04,011 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-794010729/tmp2110365162/antlr-runtime-3.4.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/22/antlr-runtime-3.4.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:55:04,118 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-794010729/tmp-268150203/pig-0.14.0-core-h2.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/23/pig-0.14.0-core-h2.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:55:04,146 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-794010729/tmp2039871689/automaton-1.11-8.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/24/automaton-1.11-8.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:55:04,171 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0004/job.splitmetainfo(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0004/filecache/10/job.splitmetainfo) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:55:04,205 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0004/job.split(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0004/filecache/11/job.split) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:55:04,234 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0004/job.xml(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0004/filecache/12/job.xml) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 10:55:04,234 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0004_01_000001 transitioned from LOCALIZING to LOCALIZED
2015-07-29 10:55:04,288 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0004_01_000001 transitioned from LOCALIZED to RUNNING
2015-07-29 10:55:04,295 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0004/container_1438157687835_0004_01_000001/default_container_executor.sh]
2015-07-29 10:55:04,546 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1438157687835_0004_01_000001
2015-07-29 10:55:04,620 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20158 for container-id container_1438157687835_0004_01_000001: 29.1 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 10:55:07,679 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20158 for container-id container_1438157687835_0004_01_000001: 87.8 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 10:55:10,791 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20158 for container-id container_1438157687835_0004_01_000001: 110.5 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 10:55:13,858 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20158 for container-id container_1438157687835_0004_01_000001: 125.7 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 10:55:16,949 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20158 for container-id container_1438157687835_0004_01_000001: 135.7 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:55:20,024 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20158 for container-id container_1438157687835_0004_01_000001: 140.8 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:55:21,136 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0004_000001 (auth:SIMPLE)
2015-07-29 10:55:21,151 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1438157687835_0004_01_000002 by user nadir
2015-07-29 10:55:21,151 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1438157687835_0004_01_000002 to application application_1438157687835_0004
2015-07-29 10:55:21,152 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0004_01_000002 transitioned from NEW to LOCALIZING
2015-07-29 10:55:21,152 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1438157687835_0004
2015-07-29 10:55:21,152 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_INIT for appId application_1438157687835_0004
2015-07-29 10:55:21,152 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got APPLICATION_INIT for service mapreduce_shuffle
2015-07-29 10:55:21,162 INFO org.apache.hadoop.mapred.ShuffleHandler: Added token for job_1438157687835_0004
2015-07-29 10:55:21,163 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0004_01_000002 transitioned from LOCALIZING to LOCALIZED
2015-07-29 10:55:21,165 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0004	CONTAINERID=container_1438157687835_0004_01_000002
2015-07-29 10:55:21,311 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0004_01_000002 transitioned from LOCALIZED to RUNNING
2015-07-29 10:55:21,327 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0004/container_1438157687835_0004_01_000002/default_container_executor.sh]
2015-07-29 10:55:23,024 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1438157687835_0004_01_000002
2015-07-29 10:55:23,091 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20259 for container-id container_1438157687835_0004_01_000002: 66.9 MB of 1 GB physical memory used; 724.6 MB of 2.1 GB virtual memory used
2015-07-29 10:55:23,160 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20158 for container-id container_1438157687835_0004_01_000001: 144.8 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:55:26,252 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20259 for container-id container_1438157687835_0004_01_000002: 95.4 MB of 1 GB physical memory used; 730.2 MB of 2.1 GB virtual memory used
2015-07-29 10:55:26,346 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20158 for container-id container_1438157687835_0004_01_000001: 144.8 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:55:29,410 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20259 for container-id container_1438157687835_0004_01_000002: 119.1 MB of 1 GB physical memory used; 757.8 MB of 2.1 GB virtual memory used
2015-07-29 10:55:29,526 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20158 for container-id container_1438157687835_0004_01_000001: 144.9 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:55:32,593 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20259 for container-id container_1438157687835_0004_01_000002: 128.6 MB of 1 GB physical memory used; 760.9 MB of 2.1 GB virtual memory used
2015-07-29 10:55:32,642 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20158 for container-id container_1438157687835_0004_01_000001: 144.9 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:55:35,553 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0004_000001 (auth:SIMPLE)
2015-07-29 10:55:35,575 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Stopping container with container Id: container_1438157687835_0004_01_000002
2015-07-29 10:55:35,575 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0004	CONTAINERID=container_1438157687835_0004_01_000002
2015-07-29 10:55:35,576 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0004_01_000002 transitioned from RUNNING to KILLING
2015-07-29 10:55:35,576 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1438157687835_0004_01_000002
2015-07-29 10:55:35,635 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1438157687835_0004_01_000002 is : 143
2015-07-29 10:55:35,771 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20259 for container-id container_1438157687835_0004_01_000002: -1B of 1 GB physical memory used; -1B of 2.1 GB virtual memory used
2015-07-29 10:55:35,801 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0004_01_000002 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
2015-07-29 10:55:35,807 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0004/container_1438157687835_0004_01_000002
2015-07-29 10:55:35,813 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1438157687835_0004	CONTAINERID=container_1438157687835_0004_01_000002
2015-07-29 10:55:35,813 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0004_01_000002 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
2015-07-29 10:55:35,815 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1438157687835_0004_01_000002 from application application_1438157687835_0004
2015-07-29 10:55:35,815 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1438157687835_0004
2015-07-29 10:55:35,846 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20158 for container-id container_1438157687835_0004_01_000001: 145.0 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:55:38,393 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0004_000001 (auth:SIMPLE)
2015-07-29 10:55:38,408 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1438157687835_0004_01_000003 by user nadir
2015-07-29 10:55:38,408 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1438157687835_0004_01_000003 to application application_1438157687835_0004
2015-07-29 10:55:38,409 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0004_01_000003 transitioned from NEW to LOCALIZING
2015-07-29 10:55:38,409 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1438157687835_0004
2015-07-29 10:55:38,409 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_INIT for appId application_1438157687835_0004
2015-07-29 10:55:38,409 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got APPLICATION_INIT for service mapreduce_shuffle
2015-07-29 10:55:38,409 INFO org.apache.hadoop.mapred.ShuffleHandler: Added token for job_1438157687835_0004
2015-07-29 10:55:38,411 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0004_01_000003 transitioned from LOCALIZING to LOCALIZED
2015-07-29 10:55:38,411 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0004	CONTAINERID=container_1438157687835_0004_01_000003
2015-07-29 10:55:38,466 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0004_01_000003 transitioned from LOCALIZED to RUNNING
2015-07-29 10:55:38,474 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0004/container_1438157687835_0004_01_000003/default_container_executor.sh]
2015-07-29 10:55:38,596 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1438157687835_0004_01_000002]
2015-07-29 10:55:38,846 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1438157687835_0004_01_000003
2015-07-29 10:55:38,846 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1438157687835_0004_01_000002
2015-07-29 10:55:38,896 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20158 for container-id container_1438157687835_0004_01_000001: 145.0 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:55:38,947 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20319 for container-id container_1438157687835_0004_01_000003: 35.4 MB of 1 GB physical memory used; 697.8 MB of 2.1 GB virtual memory used
2015-07-29 10:55:42,003 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20158 for container-id container_1438157687835_0004_01_000001: 145.0 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:55:42,060 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20319 for container-id container_1438157687835_0004_01_000003: 94.4 MB of 1 GB physical memory used; 731.4 MB of 2.1 GB virtual memory used
2015-07-29 10:55:45,133 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20158 for container-id container_1438157687835_0004_01_000001: 145.0 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:55:45,182 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20319 for container-id container_1438157687835_0004_01_000003: 117.6 MB of 1 GB physical memory used; 757.7 MB of 2.1 GB virtual memory used
2015-07-29 10:55:48,239 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20158 for container-id container_1438157687835_0004_01_000001: 145.0 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:55:48,313 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20319 for container-id container_1438157687835_0004_01_000003: 137.9 MB of 1 GB physical memory used; 760.8 MB of 2.1 GB virtual memory used
2015-07-29 10:55:49,053 INFO org.apache.hadoop.mapred.ShuffleHandler: Setting connection close header...
2015-07-29 10:55:51,308 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0004_000001 (auth:SIMPLE)
2015-07-29 10:55:51,337 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Stopping container with container Id: container_1438157687835_0004_01_000003
2015-07-29 10:55:51,338 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0004	CONTAINERID=container_1438157687835_0004_01_000003
2015-07-29 10:55:51,338 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0004_01_000003 transitioned from RUNNING to KILLING
2015-07-29 10:55:51,339 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1438157687835_0004_01_000003
2015-07-29 10:55:51,454 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1438157687835_0004_01_000003 is : 143
2015-07-29 10:55:51,609 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20158 for container-id container_1438157687835_0004_01_000001: 145.1 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:55:51,746 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0004_01_000003 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
2015-07-29 10:55:51,758 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0004/container_1438157687835_0004_01_000003
2015-07-29 10:55:51,764 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1438157687835_0004	CONTAINERID=container_1438157687835_0004_01_000003
2015-07-29 10:55:51,768 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0004_01_000003 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
2015-07-29 10:55:51,768 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1438157687835_0004_01_000003 from application application_1438157687835_0004
2015-07-29 10:55:51,768 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1438157687835_0004
2015-07-29 10:55:51,796 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20319 for container-id container_1438157687835_0004_01_000003: -1B of 1 GB physical memory used; -1B of 2.1 GB virtual memory used
2015-07-29 10:55:54,796 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1438157687835_0004_01_000003
2015-07-29 10:55:54,867 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20158 for container-id container_1438157687835_0004_01_000001: 146.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:55:57,896 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 20158 for container-id container_1438157687835_0004_01_000001: 146.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 10:55:58,204 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Container container_1438157687835_0004_01_000001 succeeded 
2015-07-29 10:55:58,205 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0004_01_000001 transitioned from RUNNING to EXITED_WITH_SUCCESS
2015-07-29 10:55:58,205 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1438157687835_0004_01_000001
2015-07-29 10:55:58,302 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0004/container_1438157687835_0004_01_000001
2015-07-29 10:55:58,305 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1438157687835_0004	CONTAINERID=container_1438157687835_0004_01_000001
2015-07-29 10:55:58,305 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0004_01_000001 transitioned from EXITED_WITH_SUCCESS to DONE
2015-07-29 10:55:58,305 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1438157687835_0004_01_000001 from application application_1438157687835_0004
2015-07-29 10:55:58,305 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1438157687835_0004
2015-07-29 10:55:58,392 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0004_000001 (auth:SIMPLE)
2015-07-29 10:55:58,399 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Stopping container with container Id: container_1438157687835_0004_01_000001
2015-07-29 10:55:58,400 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0004	CONTAINERID=container_1438157687835_0004_01_000001
2015-07-29 10:55:58,404 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1438157687835_0004_01_000001]
2015-07-29 10:55:58,404 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0004 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP
2015-07-29 10:55:58,405 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0004
2015-07-29 10:55:58,406 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_STOP for appId application_1438157687835_0004
2015-07-29 10:55:58,407 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0004 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED
2015-07-29 10:55:58,407 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler: Scheduling Log Deletion for application: application_1438157687835_0004, with delay of 10800 seconds
2015-07-29 10:56:00,896 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1438157687835_0004_01_000001
2015-07-29 11:17:41,361 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0005_000001 (auth:SIMPLE)
2015-07-29 11:17:41,369 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1438157687835_0005_01_000001 by user nadir
2015-07-29 11:17:41,370 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Creating a new application reference for app application_1438157687835_0005
2015-07-29 11:17:41,370 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0005 transitioned from NEW to INITING
2015-07-29 11:17:41,371 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0005 transitioned from INITING to RUNNING
2015-07-29 11:17:41,371 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1438157687835_0005_01_000001 to application application_1438157687835_0005
2015-07-29 11:17:41,372 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0005_01_000001 transitioned from NEW to LOCALIZING
2015-07-29 11:17:41,372 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1438157687835_0005
2015-07-29 11:17:41,372 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp601885660/tmp790331548/joda-time-2.1.jar transitioned from INIT to DOWNLOADING
2015-07-29 11:17:41,372 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp601885660/tmp-87781152/guava-11.0.2.jar transitioned from INIT to DOWNLOADING
2015-07-29 11:17:41,372 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp601885660/tmp1921195237/antlr-runtime-3.4.jar transitioned from INIT to DOWNLOADING
2015-07-29 11:17:41,372 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp601885660/tmp-215978682/pig-0.14.0-core-h2.jar transitioned from INIT to DOWNLOADING
2015-07-29 11:17:41,372 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp601885660/tmp-8374221/automaton-1.11-8.jar transitioned from INIT to DOWNLOADING
2015-07-29 11:17:41,372 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0005/job.splitmetainfo transitioned from INIT to DOWNLOADING
2015-07-29 11:17:41,373 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0005/job.split transitioned from INIT to DOWNLOADING
2015-07-29 11:17:41,373 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0005/job.xml transitioned from INIT to DOWNLOADING
2015-07-29 11:17:41,373 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Created localizer for container_1438157687835_0005_01_000001
2015-07-29 11:17:41,373 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0005	CONTAINERID=container_1438157687835_0005_01_000001
2015-07-29 11:17:41,380 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-nadir/nm-local-dir/nmPrivate/container_1438157687835_0005_01_000001.tokens. Credentials list: 
2015-07-29 11:17:41,392 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Initializing user nadir
2015-07-29 11:17:41,395 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-nadir/nm-local-dir/nmPrivate/container_1438157687835_0005_01_000001.tokens to /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0005/container_1438157687835_0005_01_000001.tokens
2015-07-29 11:17:41,396 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0005 = file:/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0005
2015-07-29 11:17:41,493 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp601885660/tmp790331548/joda-time-2.1.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/25/joda-time-2.1.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 11:17:41,546 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp601885660/tmp-87781152/guava-11.0.2.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/26/guava-11.0.2.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 11:17:41,574 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp601885660/tmp1921195237/antlr-runtime-3.4.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/27/antlr-runtime-3.4.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 11:17:41,653 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp601885660/tmp-215978682/pig-0.14.0-core-h2.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/28/pig-0.14.0-core-h2.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 11:17:41,680 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp601885660/tmp-8374221/automaton-1.11-8.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/29/automaton-1.11-8.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 11:17:41,709 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0005/job.splitmetainfo(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0005/filecache/10/job.splitmetainfo) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 11:17:41,745 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0005/job.split(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0005/filecache/11/job.split) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 11:17:41,783 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0005/job.xml(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0005/filecache/12/job.xml) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 11:17:41,784 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0005_01_000001 transitioned from LOCALIZING to LOCALIZED
2015-07-29 11:17:41,863 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0005_01_000001 transitioned from LOCALIZED to RUNNING
2015-07-29 11:17:41,873 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0005/container_1438157687835_0005_01_000001/default_container_executor.sh]
2015-07-29 11:17:42,954 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1438157687835_0005_01_000001
2015-07-29 11:17:42,994 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 62.6 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 11:17:46,030 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 100.0 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 11:17:49,067 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 117.9 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 11:17:52,132 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 132.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:17:55,149 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 143.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:17:56,388 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0005_000001 (auth:SIMPLE)
2015-07-29 11:17:56,396 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1438157687835_0005_01_000002 by user nadir
2015-07-29 11:17:56,396 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1438157687835_0005_01_000002 to application application_1438157687835_0005
2015-07-29 11:17:56,397 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0005_01_000002 transitioned from NEW to LOCALIZING
2015-07-29 11:17:56,397 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1438157687835_0005
2015-07-29 11:17:56,397 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_INIT for appId application_1438157687835_0005
2015-07-29 11:17:56,397 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got APPLICATION_INIT for service mapreduce_shuffle
2015-07-29 11:17:56,397 INFO org.apache.hadoop.mapred.ShuffleHandler: Added token for job_1438157687835_0005
2015-07-29 11:17:56,399 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0005_01_000002 transitioned from LOCALIZING to LOCALIZED
2015-07-29 11:17:56,400 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0005	CONTAINERID=container_1438157687835_0005_01_000002
2015-07-29 11:17:56,482 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0005_01_000002 transitioned from LOCALIZED to RUNNING
2015-07-29 11:17:56,489 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0005/container_1438157687835_0005_01_000002/default_container_executor.sh]
2015-07-29 11:17:58,150 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1438157687835_0005_01_000002
2015-07-29 11:17:58,187 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 78.7 MB of 1 GB physical memory used; 724.4 MB of 2.1 GB virtual memory used
2015-07-29 11:17:58,226 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.2 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:01,308 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 108.0 MB of 1 GB physical memory used; 732.0 MB of 2.1 GB virtual memory used
2015-07-29 11:18:01,348 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.2 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:04,381 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 130.9 MB of 1 GB physical memory used; 759.8 MB of 2.1 GB virtual memory used
2015-07-29 11:18:04,421 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:07,474 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 244.9 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:18:07,511 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:10,545 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 246.2 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:18:10,581 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:13,615 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 246.3 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:18:13,658 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:16,697 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 246.3 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:18:16,731 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:19,762 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 246.3 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:18:19,801 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:22,843 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 246.3 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:18:22,877 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:25,908 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 246.3 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:18:25,951 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:28,993 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 246.3 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:18:29,065 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:32,118 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 246.3 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:18:32,176 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:35,235 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 246.5 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:18:35,310 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:38,346 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 293.4 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:18:38,385 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:41,419 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 293.4 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:18:41,455 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:44,503 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 293.7 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:18:44,540 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:47,585 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 293.7 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:18:47,625 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:50,662 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 293.7 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:18:50,709 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:53,750 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 293.7 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:18:53,788 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:56,821 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 293.7 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:18:56,875 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:18:59,938 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 293.7 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:18:59,975 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:19:03,004 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 293.7 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:19:03,043 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:19:06,087 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 301.1 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:19:06,123 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:19:09,167 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 301.1 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:19:09,222 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:19:12,269 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 303.4 MB of 1 GB physical memory used; 763.7 MB of 2.1 GB virtual memory used
2015-07-29 11:19:12,336 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:19:15,373 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22051 for container-id container_1438157687835_0005_01_000002: 303.3 MB of 1 GB physical memory used; 764.7 MB of 2.1 GB virtual memory used
2015-07-29 11:19:15,421 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:19:18,033 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0005_000001 (auth:SIMPLE)
2015-07-29 11:19:18,040 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Stopping container with container Id: container_1438157687835_0005_01_000002
2015-07-29 11:19:18,040 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0005	CONTAINERID=container_1438157687835_0005_01_000002
2015-07-29 11:19:18,041 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0005_01_000002 transitioned from RUNNING to KILLING
2015-07-29 11:19:18,041 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1438157687835_0005_01_000002
2015-07-29 11:19:18,214 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1438157687835_0005_01_000002 is : 143
2015-07-29 11:19:18,214 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0005_01_000002 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
2015-07-29 11:19:18,215 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0005/container_1438157687835_0005_01_000002
2015-07-29 11:19:18,217 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1438157687835_0005	CONTAINERID=container_1438157687835_0005_01_000002
2015-07-29 11:19:18,218 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0005_01_000002 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
2015-07-29 11:19:18,218 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1438157687835_0005_01_000002 from application application_1438157687835_0005
2015-07-29 11:19:18,218 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1438157687835_0005
2015-07-29 11:19:18,428 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1438157687835_0005_01_000002
2015-07-29 11:19:18,444 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.5 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:19:20,298 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0005_000001 (auth:SIMPLE)
2015-07-29 11:19:20,316 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1438157687835_0005_01_000003 by user nadir
2015-07-29 11:19:20,317 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1438157687835_0005_01_000003 to application application_1438157687835_0005
2015-07-29 11:19:20,318 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0005_01_000003 transitioned from NEW to LOCALIZING
2015-07-29 11:19:20,320 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1438157687835_0005
2015-07-29 11:19:20,320 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_INIT for appId application_1438157687835_0005
2015-07-29 11:19:20,320 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got APPLICATION_INIT for service mapreduce_shuffle
2015-07-29 11:19:20,320 INFO org.apache.hadoop.mapred.ShuffleHandler: Added token for job_1438157687835_0005
2015-07-29 11:19:20,321 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0005	CONTAINERID=container_1438157687835_0005_01_000003
2015-07-29 11:19:20,327 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0005_01_000003 transitioned from LOCALIZING to LOCALIZED
2015-07-29 11:19:20,401 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0005_01_000003 transitioned from LOCALIZED to RUNNING
2015-07-29 11:19:20,413 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0005/container_1438157687835_0005_01_000003/default_container_executor.sh]
2015-07-29 11:19:21,056 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1438157687835_0005_01_000002]
2015-07-29 11:19:21,444 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1438157687835_0005_01_000003
2015-07-29 11:19:21,497 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22121 for container-id container_1438157687835_0005_01_000003: 53.2 MB of 1 GB physical memory used; 724.4 MB of 2.1 GB virtual memory used
2015-07-29 11:19:21,547 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.5 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:19:24,593 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22121 for container-id container_1438157687835_0005_01_000003: 86.2 MB of 1 GB physical memory used; 724.4 MB of 2.1 GB virtual memory used
2015-07-29 11:19:24,640 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.5 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:19:27,682 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22121 for container-id container_1438157687835_0005_01_000003: 106.0 MB of 1 GB physical memory used; 732.0 MB of 2.1 GB virtual memory used
2015-07-29 11:19:27,731 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.5 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:19:30,781 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22121 for container-id container_1438157687835_0005_01_000003: 118.2 MB of 1 GB physical memory used; 757.7 MB of 2.1 GB virtual memory used
2015-07-29 11:19:30,838 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.5 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:19:33,893 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22121 for container-id container_1438157687835_0005_01_000003: 132.9 MB of 1 GB physical memory used; 758.7 MB of 2.1 GB virtual memory used
2015-07-29 11:19:33,946 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.5 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:19:36,055 INFO org.apache.hadoop.mapred.ShuffleHandler: Setting connection close header...
2015-07-29 11:19:36,976 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22121 for container-id container_1438157687835_0005_01_000003: 136.4 MB of 1 GB physical memory used; 769.8 MB of 2.1 GB virtual memory used
2015-07-29 11:19:37,010 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.5 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:19:40,042 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22121 for container-id container_1438157687835_0005_01_000003: 138.8 MB of 1 GB physical memory used; 769.8 MB of 2.1 GB virtual memory used
2015-07-29 11:19:40,085 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.5 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:19:43,131 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22121 for container-id container_1438157687835_0005_01_000003: 139.9 MB of 1 GB physical memory used; 769.8 MB of 2.1 GB virtual memory used
2015-07-29 11:19:43,181 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.5 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:19:46,225 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 22121 for container-id container_1438157687835_0005_01_000003: 149.7 MB of 1 GB physical memory used; 769.8 MB of 2.1 GB virtual memory used
2015-07-29 11:19:46,289 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.5 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:19:47,706 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0005_000001 (auth:SIMPLE)
2015-07-29 11:19:47,717 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Stopping container with container Id: container_1438157687835_0005_01_000003
2015-07-29 11:19:47,717 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0005	CONTAINERID=container_1438157687835_0005_01_000003
2015-07-29 11:19:47,718 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0005_01_000003 transitioned from RUNNING to KILLING
2015-07-29 11:19:47,718 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1438157687835_0005_01_000003
2015-07-29 11:19:47,747 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1438157687835_0005_01_000003 is : 143
2015-07-29 11:19:47,829 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0005_01_000003 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
2015-07-29 11:19:47,830 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0005/container_1438157687835_0005_01_000003
2015-07-29 11:19:47,832 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1438157687835_0005	CONTAINERID=container_1438157687835_0005_01_000003
2015-07-29 11:19:47,832 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0005_01_000003 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
2015-07-29 11:19:47,833 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1438157687835_0005_01_000003 from application application_1438157687835_0005
2015-07-29 11:19:47,833 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1438157687835_0005
2015-07-29 11:19:49,294 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1438157687835_0005_01_000003
2015-07-29 11:19:49,310 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 150.3 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:19:52,360 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 21952 for container-id container_1438157687835_0005_01_000001: 149.1 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:19:54,456 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Container container_1438157687835_0005_01_000001 succeeded 
2015-07-29 11:19:54,457 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0005_01_000001 transitioned from RUNNING to EXITED_WITH_SUCCESS
2015-07-29 11:19:54,457 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1438157687835_0005_01_000001
2015-07-29 11:19:54,583 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0005/container_1438157687835_0005_01_000001
2015-07-29 11:19:54,590 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1438157687835_0005	CONTAINERID=container_1438157687835_0005_01_000001
2015-07-29 11:19:54,590 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0005_01_000001 transitioned from EXITED_WITH_SUCCESS to DONE
2015-07-29 11:19:54,590 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1438157687835_0005_01_000001 from application application_1438157687835_0005
2015-07-29 11:19:54,590 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1438157687835_0005
2015-07-29 11:19:55,064 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0005_000001 (auth:SIMPLE)
2015-07-29 11:19:55,079 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Stopping container with container Id: container_1438157687835_0005_01_000001
2015-07-29 11:19:55,079 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0005	CONTAINERID=container_1438157687835_0005_01_000001
2015-07-29 11:19:55,083 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1438157687835_0005_01_000001]
2015-07-29 11:19:55,083 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0005 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP
2015-07-29 11:19:55,083 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0005
2015-07-29 11:19:55,098 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_STOP for appId application_1438157687835_0005
2015-07-29 11:19:55,099 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0005 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED
2015-07-29 11:19:55,099 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler: Scheduling Log Deletion for application: application_1438157687835_0005, with delay of 10800 seconds
2015-07-29 11:19:55,360 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1438157687835_0005_01_000001
2015-07-29 11:20:55,623 INFO org.apache.hadoop.http.HttpServer2: Process Thread Dump: jsp requested
72 active threads
Thread 323 (647005846@qtp-52154594-6):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1
  Stack:
    java.lang.Object.wait(Native Method)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
Thread 322 (1422946729@qtp-52154594-5):
  State: RUNNABLE
  Blocked count: 16
  Waited count: 15
  Stack:
    sun.management.ThreadImpl.getThreadInfo1(Native Method)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:174)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:139)
    org.apache.hadoop.util.ReflectionUtils.printThreadInfo(ReflectionUtils.java:168)
    org.apache.hadoop.util.ReflectionUtils.logThreadInfo(ReflectionUtils.java:223)
    org.apache.hadoop.http.HttpServer2$StackServlet.doGet(HttpServer2.java:1114)
    javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
    javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
    org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
    com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:66)
    com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)
    com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)
    com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)
    com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)
    com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)
    com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)
    com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)
    org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
    org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
Thread 321 (2121755270@qtp-52154594-4):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 1
  Stack:
    java.lang.Object.wait(Native Method)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
Thread 320 (60471482@qtp-52154594-3):
  State: TIMED_WAITING
  Blocked count: 4
  Waited count: 5
  Stack:
    java.lang.Object.wait(Native Method)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
Thread 319 (1631178287@qtp-52154594-2):
  State: TIMED_WAITING
  Blocked count: 4
  Waited count: 4
  Stack:
    java.lang.Object.wait(Native Method)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
Thread 318 (LogDeleter #3):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3d2500ae
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1085)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:745)
Thread 309 (Readahead Thread #2):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@6b7c05e8
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:374)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:745)
Thread 256 (LogDeleter #2):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3d2500ae
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1085)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:745)
Thread 248 (Readahead Thread #1):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@6b7c05e8
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:374)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:745)
Thread 194 (LogDeleter #1):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3d2500ae
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1085)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:745)
Thread 141 (LogDeleter #0):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:745)
Thread 135 (DeletionService #3):
  State: WAITING
  Blocked count: 0
  Waited count: 8
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@30e51012
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1079)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:745)
Thread 131 (Readahead Thread #0):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@6b7c05e8
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:374)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:745)
Thread 127 (DeletionService #2):
  State: WAITING
  Blocked count: 0
  Waited count: 9
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@30e51012
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1079)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:745)
Thread 126 (DeletionService #1):
  State: WAITING
  Blocked count: 0
  Waited count: 9
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@30e51012
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1079)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:745)
Thread 118 (DeletionService #0):
  State: WAITING
  Blocked count: 0
  Waited count: 11
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@30e51012
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1079)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:745)
Thread 83 (org.apache.hadoop.hdfs.PeerCache@5172e27d):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 802
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hdfs.PeerCache.run(PeerCache.java:255)
    org.apache.hadoop.hdfs.PeerCache.access$000(PeerCache.java:46)
    org.apache.hadoop.hdfs.PeerCache$1.run(PeerCache.java:124)
    java.lang.Thread.run(Thread.java:745)
Thread 81 (client DomainSocketWatcher):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    org.apache.hadoop.net.unix.DomainSocketWatcher.doPoll0(Native Method)
    org.apache.hadoop.net.unix.DomainSocketWatcher.access$900(DomainSocketWatcher.java:52)
    org.apache.hadoop.net.unix.DomainSocketWatcher$2.run(DomainSocketWatcher.java:511)
    java.lang.Thread.run(Thread.java:745)
Thread 73 (DestroyJavaVM):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
Thread 72 (Node Status Updater):
  State: TIMED_WAITING
  Blocked count: 3839
  Waited count: 8053
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$1.run(NodeStatusUpdaterImpl.java:674)
    java.lang.Thread.run(Thread.java:745)
Thread 71 (IPC Parameter Sending Thread #0):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 4065
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
    java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)
    java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:745)
Thread 70 (IPC Client (87900807) connection to /0.0.0.0:8031 from nadir):
  State: TIMED_WAITING
  Blocked count: 3953
  Waited count: 3952
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:928)
    org.apache.hadoop.ipc.Client$Connection.run(Client.java:973)
Thread 69 (AsyncDispatcher event handler):
  State: WAITING
  Blocked count: 0
  Waited count: 5
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@4f345357
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:100)
    java.lang.Thread.run(Thread.java:745)
Thread 68 (com.google.inject.internal.util.$Finalizer):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.lang.ref.ReferenceQueue$Lock@10494546
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)
    com.google.inject.internal.util.$Finalizer.run(Finalizer.java:114)
Thread 67 (Timer-0):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 133
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 66 (586691118@qtp-52154594-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8042):
  State: RUNNABLE
  Blocked count: 45
  Waited count: 1
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
    org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
    org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
    org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
    org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Thread 64 (pool-10-thread-1):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:745)
Thread 62 (AsyncDispatcher event handler):
  State: WAITING
  Blocked count: 28
  Waited count: 107
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@61616b58
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:100)
    java.lang.Thread.run(Thread.java:745)
Thread 12 (Container Monitor):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1319
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.run(ContainersMonitorImpl.java:535)
Thread 20 (Public Localizer):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@52feae2
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
    java.util.concurrent.ExecutorCompletionService.take(ExecutorCompletionService.java:193)
    org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$PublicLocalizer.run(ResourceLocalizationService.java:851)
Thread 61 (IPC Server handler 4 on 8040):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3972
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 60 (IPC Server handler 3 on 8040):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3973
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 59 (IPC Server handler 2 on 8040):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3970
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 58 (IPC Server handler 1 on 8040):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3970
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 57 (IPC Server handler 0 on 8040):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3970
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 53 (IPC Server listener on 8040):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:102)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:682)
Thread 56 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
    org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:856)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:839)
Thread 55 (IPC Server idle connection scanner for port 8040):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 398
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 54 (Socket Reader #1 for port 8040):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:102)
    org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:629)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
Thread 52 (ResourceLocalizationService Cache Cleanup):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 7
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
    java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
    java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:745)
Thread 50 (IPC Server handler 19 on 60035):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3965
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 49 (IPC Server handler 18 on 60035):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3966
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 48 (IPC Server handler 17 on 60035):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3967
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 47 (IPC Server handler 16 on 60035):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3965
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 46 (IPC Server handler 15 on 60035):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3967
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 45 (IPC Server handler 14 on 60035):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3965
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 44 (IPC Server handler 13 on 60035):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3966
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 43 (IPC Server handler 12 on 60035):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3966
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 42 (IPC Server handler 11 on 60035):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3966
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 41 (IPC Server handler 10 on 60035):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3966
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 40 (IPC Server handler 9 on 60035):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3967
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 39 (IPC Server handler 8 on 60035):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3966
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 38 (IPC Server handler 7 on 60035):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3966
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 37 (IPC Server handler 6 on 60035):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3965
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 36 (IPC Server handler 5 on 60035):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3969
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 35 (IPC Server handler 4 on 60035):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3966
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 34 (IPC Server handler 3 on 60035):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3965
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 33 (IPC Server handler 2 on 60035):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3968
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 32 (IPC Server handler 1 on 60035):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 3969
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 31 (IPC Server handler 0 on 60035):
  State: TIMED_WAITING
  Blocked count: 3
  Waited count: 3965
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
    java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
    org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:109)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:2017)
Thread 27 (IPC Server listener on 60035):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:102)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:682)
Thread 30 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
    org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:856)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:839)
Thread 29 (IPC Server idle connection scanner for port 60035):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 398
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 28 (Socket Reader #1 for port 60035):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:102)
    org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:629)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:608)
Thread 26 (DiskHealthMonitor-Timer):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 34
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 23 (New I/O server boss #3):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:102)
    org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
    org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
    org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
    org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:745)
Thread 22 (New I/O worker #2):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
    org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
    org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
    org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
    org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
    org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:745)
Thread 21 (New I/O worker #1):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
    org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
    org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
    org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
    org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
    org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:745)
Thread 13 (Timer for 'NodeManager' metrics system):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 397
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:552)
    java.util.TimerThread.run(Timer.java:505)
Thread 4 (Signal Dispatcher):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
Thread 3 (Finalizer):
  State: WAITING
  Blocked count: 72
  Waited count: 73
  Waiting on java.lang.ref.ReferenceQueue$Lock@6c97a376
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)
    java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)
Thread 2 (Reference Handler):
  State: WAITING
  Blocked count: 60
  Waited count: 61
  Waiting on java.lang.ref.Reference$Lock@576ccfac
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:503)
    java.lang.ref.Reference$ReferenceHandler.run(Reference.java:133)

2015-07-29 11:54:17,523 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0006_000001 (auth:SIMPLE)
2015-07-29 11:54:17,537 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1438157687835_0006_01_000001 by user nadir
2015-07-29 11:54:17,540 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Creating a new application reference for app application_1438157687835_0006
2015-07-29 11:54:17,540 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0006 transitioned from NEW to INITING
2015-07-29 11:54:17,540 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0006 transitioned from INITING to RUNNING
2015-07-29 11:54:17,541 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1438157687835_0006_01_000001 to application application_1438157687835_0006
2015-07-29 11:54:17,541 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0006_01_000001 transitioned from NEW to LOCALIZING
2015-07-29 11:54:17,542 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1438157687835_0006
2015-07-29 11:54:17,542 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp60813819/tmp-1537770896/joda-time-2.1.jar transitioned from INIT to DOWNLOADING
2015-07-29 11:54:17,542 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp60813819/tmp1700556739/guava-11.0.2.jar transitioned from INIT to DOWNLOADING
2015-07-29 11:54:17,542 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp60813819/tmp1500249756/antlr-runtime-3.4.jar transitioned from INIT to DOWNLOADING
2015-07-29 11:54:17,542 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp60813819/tmp-1754458621/pig-0.14.0-core-h2.jar transitioned from INIT to DOWNLOADING
2015-07-29 11:54:17,542 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp60813819/tmp770867802/automaton-1.11-8.jar transitioned from INIT to DOWNLOADING
2015-07-29 11:54:17,542 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0006/job.splitmetainfo transitioned from INIT to DOWNLOADING
2015-07-29 11:54:17,542 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0006/job.split transitioned from INIT to DOWNLOADING
2015-07-29 11:54:17,543 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0006/job.xml transitioned from INIT to DOWNLOADING
2015-07-29 11:54:17,543 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Created localizer for container_1438157687835_0006_01_000001
2015-07-29 11:54:17,545 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0006	CONTAINERID=container_1438157687835_0006_01_000001
2015-07-29 11:54:17,551 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-nadir/nm-local-dir/nmPrivate/container_1438157687835_0006_01_000001.tokens. Credentials list: 
2015-07-29 11:54:17,564 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Initializing user nadir
2015-07-29 11:54:17,567 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-nadir/nm-local-dir/nmPrivate/container_1438157687835_0006_01_000001.tokens to /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0006/container_1438157687835_0006_01_000001.tokens
2015-07-29 11:54:17,568 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0006 = file:/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0006
2015-07-29 11:54:17,686 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp60813819/tmp-1537770896/joda-time-2.1.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/30/joda-time-2.1.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 11:54:17,785 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp60813819/tmp1700556739/guava-11.0.2.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/31/guava-11.0.2.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 11:54:17,812 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp60813819/tmp1500249756/antlr-runtime-3.4.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/32/antlr-runtime-3.4.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 11:54:17,897 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp60813819/tmp-1754458621/pig-0.14.0-core-h2.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/33/pig-0.14.0-core-h2.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 11:54:17,926 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp60813819/tmp770867802/automaton-1.11-8.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/34/automaton-1.11-8.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 11:54:17,956 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0006/job.splitmetainfo(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0006/filecache/10/job.splitmetainfo) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 11:54:17,995 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0006/job.split(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0006/filecache/11/job.split) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 11:54:18,027 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0006/job.xml(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0006/filecache/12/job.xml) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 11:54:18,030 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0006_01_000001 transitioned from LOCALIZING to LOCALIZED
2015-07-29 11:54:18,078 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0006_01_000001 transitioned from LOCALIZED to RUNNING
2015-07-29 11:54:18,085 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0006/container_1438157687835_0006_01_000001/default_container_executor.sh]
2015-07-29 11:54:19,592 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1438157687835_0006_01_000001
2015-07-29 11:54:19,657 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 74.1 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 11:54:22,704 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 105.2 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 11:54:25,751 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 118.6 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 11:54:28,802 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 132.1 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:54:31,833 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 141.9 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:54:32,837 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0006_000001 (auth:SIMPLE)
2015-07-29 11:54:32,845 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1438157687835_0006_01_000002 by user nadir
2015-07-29 11:54:32,845 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1438157687835_0006_01_000002 to application application_1438157687835_0006
2015-07-29 11:54:32,846 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0006_01_000002 transitioned from NEW to LOCALIZING
2015-07-29 11:54:32,846 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1438157687835_0006
2015-07-29 11:54:32,846 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_INIT for appId application_1438157687835_0006
2015-07-29 11:54:32,846 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got APPLICATION_INIT for service mapreduce_shuffle
2015-07-29 11:54:32,846 INFO org.apache.hadoop.mapred.ShuffleHandler: Added token for job_1438157687835_0006
2015-07-29 11:54:32,847 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0006_01_000002 transitioned from LOCALIZING to LOCALIZED
2015-07-29 11:54:32,848 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0006	CONTAINERID=container_1438157687835_0006_01_000002
2015-07-29 11:54:32,909 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0006_01_000002 transitioned from LOCALIZED to RUNNING
2015-07-29 11:54:32,916 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0006/container_1438157687835_0006_01_000002/default_container_executor.sh]
2015-07-29 11:54:34,833 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1438157687835_0006_01_000002
2015-07-29 11:54:34,884 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24657 for container-id container_1438157687835_0006_01_000002: 73.6 MB of 1 GB physical memory used; 724.6 MB of 2.1 GB virtual memory used
2015-07-29 11:54:34,936 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 146.2 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:54:37,996 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24657 for container-id container_1438157687835_0006_01_000002: 103.8 MB of 1 GB physical memory used; 732.2 MB of 2.1 GB virtual memory used
2015-07-29 11:54:38,047 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 146.2 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:54:41,097 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24657 for container-id container_1438157687835_0006_01_000002: 128.1 MB of 1 GB physical memory used; 758.9 MB of 2.1 GB virtual memory used
2015-07-29 11:54:41,159 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 146.2 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:54:44,230 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24657 for container-id container_1438157687835_0006_01_000002: 239.1 MB of 1 GB physical memory used; 763.9 MB of 2.1 GB virtual memory used
2015-07-29 11:54:44,294 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 146.2 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:54:47,351 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24657 for container-id container_1438157687835_0006_01_000002: 241.0 MB of 1 GB physical memory used; 763.9 MB of 2.1 GB virtual memory used
2015-07-29 11:54:47,408 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 146.2 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:54:50,514 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24657 for container-id container_1438157687835_0006_01_000002: 235.8 MB of 1 GB physical memory used; 763.9 MB of 2.1 GB virtual memory used
2015-07-29 11:54:50,568 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 146.2 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:54:53,700 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24657 for container-id container_1438157687835_0006_01_000002: 235.8 MB of 1 GB physical memory used; 763.9 MB of 2.1 GB virtual memory used
2015-07-29 11:54:53,798 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 146.2 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:54:56,860 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24657 for container-id container_1438157687835_0006_01_000002: 236.6 MB of 1 GB physical memory used; 763.9 MB of 2.1 GB virtual memory used
2015-07-29 11:54:56,930 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 146.2 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:54:57,997 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0006_000001 (auth:SIMPLE)
2015-07-29 11:54:58,010 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Stopping container with container Id: container_1438157687835_0006_01_000002
2015-07-29 11:54:58,010 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0006	CONTAINERID=container_1438157687835_0006_01_000002
2015-07-29 11:54:58,010 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0006_01_000002 transitioned from RUNNING to KILLING
2015-07-29 11:54:58,010 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1438157687835_0006_01_000002
2015-07-29 11:54:58,062 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1438157687835_0006_01_000002 is : 143
2015-07-29 11:54:58,171 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0006_01_000002 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
2015-07-29 11:54:58,178 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0006/container_1438157687835_0006_01_000002
2015-07-29 11:54:58,180 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1438157687835_0006	CONTAINERID=container_1438157687835_0006_01_000002
2015-07-29 11:54:58,180 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0006_01_000002 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
2015-07-29 11:54:58,180 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1438157687835_0006_01_000002 from application application_1438157687835_0006
2015-07-29 11:54:58,180 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1438157687835_0006
2015-07-29 11:54:59,931 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1438157687835_0006_01_000002
2015-07-29 11:54:59,953 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 146.6 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:55:00,679 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0006_000001 (auth:SIMPLE)
2015-07-29 11:55:00,690 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1438157687835_0006_01_000003 by user nadir
2015-07-29 11:55:00,693 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1438157687835_0006_01_000003 to application application_1438157687835_0006
2015-07-29 11:55:00,693 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0006_01_000003 transitioned from NEW to LOCALIZING
2015-07-29 11:55:00,693 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1438157687835_0006
2015-07-29 11:55:00,694 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_INIT for appId application_1438157687835_0006
2015-07-29 11:55:00,694 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got APPLICATION_INIT for service mapreduce_shuffle
2015-07-29 11:55:00,694 INFO org.apache.hadoop.mapred.ShuffleHandler: Added token for job_1438157687835_0006
2015-07-29 11:55:00,695 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0006_01_000003 transitioned from LOCALIZING to LOCALIZED
2015-07-29 11:55:00,695 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0006	CONTAINERID=container_1438157687835_0006_01_000003
2015-07-29 11:55:00,755 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0006_01_000003 transitioned from LOCALIZED to RUNNING
2015-07-29 11:55:00,762 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0006/container_1438157687835_0006_01_000003/default_container_executor.sh]
2015-07-29 11:55:01,027 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1438157687835_0006_01_000002]
2015-07-29 11:55:02,953 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1438157687835_0006_01_000003
2015-07-29 11:55:03,003 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24723 for container-id container_1438157687835_0006_01_000003: 79.6 MB of 1 GB physical memory used; 724.4 MB of 2.1 GB virtual memory used
2015-07-29 11:55:03,057 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 146.6 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:55:06,127 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24723 for container-id container_1438157687835_0006_01_000003: 106.5 MB of 1 GB physical memory used; 753.2 MB of 2.1 GB virtual memory used
2015-07-29 11:55:06,178 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 146.6 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:55:09,226 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24723 for container-id container_1438157687835_0006_01_000003: 130.4 MB of 1 GB physical memory used; 758.7 MB of 2.1 GB virtual memory used
2015-07-29 11:55:09,274 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 146.6 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:55:10,886 INFO org.apache.hadoop.mapred.ShuffleHandler: Setting connection close header...
2015-07-29 11:55:12,325 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24723 for container-id container_1438157687835_0006_01_000003: 134.5 MB of 1 GB physical memory used; 769.8 MB of 2.1 GB virtual memory used
2015-07-29 11:55:12,383 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 146.6 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:55:15,459 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24723 for container-id container_1438157687835_0006_01_000003: 135.2 MB of 1 GB physical memory used; 769.8 MB of 2.1 GB virtual memory used
2015-07-29 11:55:15,526 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 146.6 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:55:17,393 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0006_000001 (auth:SIMPLE)
2015-07-29 11:55:17,404 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Stopping container with container Id: container_1438157687835_0006_01_000003
2015-07-29 11:55:17,405 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0006	CONTAINERID=container_1438157687835_0006_01_000003
2015-07-29 11:55:17,405 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0006_01_000003 transitioned from RUNNING to KILLING
2015-07-29 11:55:17,406 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1438157687835_0006_01_000003
2015-07-29 11:55:17,508 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1438157687835_0006_01_000003 is : 143
2015-07-29 11:55:17,632 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0006_01_000003 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
2015-07-29 11:55:17,637 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0006/container_1438157687835_0006_01_000003
2015-07-29 11:55:17,643 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1438157687835_0006	CONTAINERID=container_1438157687835_0006_01_000003
2015-07-29 11:55:17,643 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0006_01_000003 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
2015-07-29 11:55:17,644 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1438157687835_0006_01_000003 from application application_1438157687835_0006
2015-07-29 11:55:17,644 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1438157687835_0006
2015-07-29 11:55:18,526 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1438157687835_0006_01_000003
2015-07-29 11:55:18,589 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 147.6 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:55:21,611 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: 148.1 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 11:55:24,502 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Container container_1438157687835_0006_01_000001 succeeded 
2015-07-29 11:55:24,502 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0006_01_000001 transitioned from RUNNING to EXITED_WITH_SUCCESS
2015-07-29 11:55:24,502 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1438157687835_0006_01_000001
2015-07-29 11:55:24,670 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0006/container_1438157687835_0006_01_000001
2015-07-29 11:55:24,675 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1438157687835_0006	CONTAINERID=container_1438157687835_0006_01_000001
2015-07-29 11:55:24,675 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0006_01_000001 transitioned from EXITED_WITH_SUCCESS to DONE
2015-07-29 11:55:24,676 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1438157687835_0006_01_000001 from application application_1438157687835_0006
2015-07-29 11:55:24,676 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1438157687835_0006
2015-07-29 11:55:24,722 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 24559 for container-id container_1438157687835_0006_01_000001: -1B of 2 GB physical memory used; -1B of 4.2 GB virtual memory used
2015-07-29 11:55:25,463 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0006_000001 (auth:SIMPLE)
2015-07-29 11:55:25,480 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Stopping container with container Id: container_1438157687835_0006_01_000001
2015-07-29 11:55:25,482 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0006	CONTAINERID=container_1438157687835_0006_01_000001
2015-07-29 11:55:25,485 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1438157687835_0006_01_000001]
2015-07-29 11:55:25,486 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0006 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP
2015-07-29 11:55:25,487 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_STOP for appId application_1438157687835_0006
2015-07-29 11:55:25,487 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0006 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED
2015-07-29 11:55:25,487 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler: Scheduling Log Deletion for application: application_1438157687835_0006, with delay of 10800 seconds
2015-07-29 11:55:25,487 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0006
2015-07-29 11:55:27,722 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1438157687835_0006_01_000001
2015-07-29 12:14:20,054 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0007_000001 (auth:SIMPLE)
2015-07-29 12:14:20,063 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1438157687835_0007_01_000001 by user nadir
2015-07-29 12:14:20,064 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Creating a new application reference for app application_1438157687835_0007
2015-07-29 12:14:20,064 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0007 transitioned from NEW to INITING
2015-07-29 12:14:20,064 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0007 transitioned from INITING to RUNNING
2015-07-29 12:14:20,065 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1438157687835_0007_01_000001 to application application_1438157687835_0007
2015-07-29 12:14:20,066 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0007_01_000001 transitioned from NEW to LOCALIZING
2015-07-29 12:14:20,066 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1438157687835_0007
2015-07-29 12:14:20,066 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1462883958/tmp856559036/joda-time-2.1.jar transitioned from INIT to DOWNLOADING
2015-07-29 12:14:20,066 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1462883958/tmp-1685507554/guava-11.0.2.jar transitioned from INIT to DOWNLOADING
2015-07-29 12:14:20,066 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1462883958/tmp-1463806561/antlr-runtime-3.4.jar transitioned from INIT to DOWNLOADING
2015-07-29 12:14:20,066 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1462883958/tmp1382234099/pig-0.14.0-core-h2.jar transitioned from INIT to DOWNLOADING
2015-07-29 12:14:20,066 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1462883958/tmp-1672067454/automaton-1.11-8.jar transitioned from INIT to DOWNLOADING
2015-07-29 12:14:20,066 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0007/job.splitmetainfo transitioned from INIT to DOWNLOADING
2015-07-29 12:14:20,067 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0007/job.split transitioned from INIT to DOWNLOADING
2015-07-29 12:14:20,067 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0007/job.xml transitioned from INIT to DOWNLOADING
2015-07-29 12:14:20,067 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Created localizer for container_1438157687835_0007_01_000001
2015-07-29 12:14:20,067 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0007	CONTAINERID=container_1438157687835_0007_01_000001
2015-07-29 12:14:20,073 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-nadir/nm-local-dir/nmPrivate/container_1438157687835_0007_01_000001.tokens. Credentials list: 
2015-07-29 12:14:20,086 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Initializing user nadir
2015-07-29 12:14:20,088 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-nadir/nm-local-dir/nmPrivate/container_1438157687835_0007_01_000001.tokens to /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0007/container_1438157687835_0007_01_000001.tokens
2015-07-29 12:14:20,089 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0007 = file:/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0007
2015-07-29 12:14:20,188 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1462883958/tmp856559036/joda-time-2.1.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/35/joda-time-2.1.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 12:14:20,234 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1462883958/tmp-1685507554/guava-11.0.2.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/36/guava-11.0.2.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 12:14:20,261 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1462883958/tmp-1463806561/antlr-runtime-3.4.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/37/antlr-runtime-3.4.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 12:14:20,370 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1462883958/tmp1382234099/pig-0.14.0-core-h2.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/38/pig-0.14.0-core-h2.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 12:14:20,440 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/temp-1462883958/tmp-1672067454/automaton-1.11-8.jar(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/filecache/39/automaton-1.11-8.jar) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 12:14:20,475 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0007/job.splitmetainfo(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0007/filecache/10/job.splitmetainfo) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 12:14:20,499 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0007/job.split(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0007/filecache/11/job.split) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 12:14:20,526 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://localhost:9000/tmp/hadoop-yarn/staging/nadir/.staging/job_1438157687835_0007/job.xml(->/tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0007/filecache/12/job.xml) transitioned from DOWNLOADING to LOCALIZED
2015-07-29 12:14:20,527 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0007_01_000001 transitioned from LOCALIZING to LOCALIZED
2015-07-29 12:14:20,617 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0007_01_000001 transitioned from LOCALIZED to RUNNING
2015-07-29 12:14:20,625 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0007/container_1438157687835_0007_01_000001/default_container_executor.sh]
2015-07-29 12:14:21,768 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1438157687835_0007_01_000001
2015-07-29 12:14:21,813 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 61.6 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 12:14:24,864 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 98.8 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 12:14:27,903 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 116.9 MB of 2 GB physical memory used; 1.5 GB of 4.2 GB virtual memory used
2015-07-29 12:14:30,951 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 133.7 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 12:14:33,975 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 145.6 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 12:14:35,136 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0007_000001 (auth:SIMPLE)
2015-07-29 12:14:35,162 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1438157687835_0007_01_000002 by user nadir
2015-07-29 12:14:35,163 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1438157687835_0007_01_000002 to application application_1438157687835_0007
2015-07-29 12:14:35,163 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0007_01_000002 transitioned from NEW to LOCALIZING
2015-07-29 12:14:35,163 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1438157687835_0007
2015-07-29 12:14:35,163 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_INIT for appId application_1438157687835_0007
2015-07-29 12:14:35,163 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got APPLICATION_INIT for service mapreduce_shuffle
2015-07-29 12:14:35,163 INFO org.apache.hadoop.mapred.ShuffleHandler: Added token for job_1438157687835_0007
2015-07-29 12:14:35,164 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0007_01_000002 transitioned from LOCALIZING to LOCALIZED
2015-07-29 12:14:35,165 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0007	CONTAINERID=container_1438157687835_0007_01_000002
2015-07-29 12:14:35,239 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0007_01_000002 transitioned from LOCALIZED to RUNNING
2015-07-29 12:14:35,246 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0007/container_1438157687835_0007_01_000002/default_container_executor.sh]
2015-07-29 12:14:36,975 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1438157687835_0007_01_000002
2015-07-29 12:14:37,036 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25478 for container-id container_1438157687835_0007_01_000002: 73.6 MB of 1 GB physical memory used; 724.6 MB of 2.1 GB virtual memory used
2015-07-29 12:14:37,080 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 149.6 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 12:14:40,126 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25478 for container-id container_1438157687835_0007_01_000002: 105.1 MB of 1 GB physical memory used; 753.4 MB of 2.1 GB virtual memory used
2015-07-29 12:14:40,169 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 149.6 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 12:14:43,220 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25478 for container-id container_1438157687835_0007_01_000002: 126.4 MB of 1 GB physical memory used; 758.9 MB of 2.1 GB virtual memory used
2015-07-29 12:14:43,271 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 149.6 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 12:14:46,314 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25478 for container-id container_1438157687835_0007_01_000002: 237.7 MB of 1 GB physical memory used; 763.9 MB of 2.1 GB virtual memory used
2015-07-29 12:14:46,358 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 149.7 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 12:14:49,397 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25478 for container-id container_1438157687835_0007_01_000002: 238.8 MB of 1 GB physical memory used; 763.9 MB of 2.1 GB virtual memory used
2015-07-29 12:14:49,460 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 149.7 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 12:14:52,501 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25478 for container-id container_1438157687835_0007_01_000002: 239.7 MB of 1 GB physical memory used; 763.9 MB of 2.1 GB virtual memory used
2015-07-29 12:14:52,573 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 149.7 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 12:14:55,609 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25478 for container-id container_1438157687835_0007_01_000002: 239.9 MB of 1 GB physical memory used; 763.9 MB of 2.1 GB virtual memory used
2015-07-29 12:14:55,645 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 149.7 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 12:14:56,705 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0007_000001 (auth:SIMPLE)
2015-07-29 12:14:56,715 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Stopping container with container Id: container_1438157687835_0007_01_000002
2015-07-29 12:14:56,715 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0007	CONTAINERID=container_1438157687835_0007_01_000002
2015-07-29 12:14:56,715 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0007_01_000002 transitioned from RUNNING to KILLING
2015-07-29 12:14:56,715 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1438157687835_0007_01_000002
2015-07-29 12:14:56,762 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1438157687835_0007_01_000002 is : 143
2015-07-29 12:14:56,904 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0007_01_000002 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
2015-07-29 12:14:56,919 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0007/container_1438157687835_0007_01_000002
2015-07-29 12:14:56,921 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1438157687835_0007	CONTAINERID=container_1438157687835_0007_01_000002
2015-07-29 12:14:56,922 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0007_01_000002 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
2015-07-29 12:14:56,922 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1438157687835_0007_01_000002 from application application_1438157687835_0007
2015-07-29 12:14:56,922 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1438157687835_0007
2015-07-29 12:14:58,645 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1438157687835_0007_01_000002
2015-07-29 12:14:58,662 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 149.8 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 12:14:58,955 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0007_000001 (auth:SIMPLE)
2015-07-29 12:14:58,963 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1438157687835_0007_01_000003 by user nadir
2015-07-29 12:14:58,963 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1438157687835_0007_01_000003 to application application_1438157687835_0007
2015-07-29 12:14:58,964 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0007_01_000003 transitioned from NEW to LOCALIZING
2015-07-29 12:14:58,964 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1438157687835_0007
2015-07-29 12:14:58,964 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_INIT for appId application_1438157687835_0007
2015-07-29 12:14:58,964 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got APPLICATION_INIT for service mapreduce_shuffle
2015-07-29 12:14:58,964 INFO org.apache.hadoop.mapred.ShuffleHandler: Added token for job_1438157687835_0007
2015-07-29 12:14:58,966 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0007_01_000003 transitioned from LOCALIZING to LOCALIZED
2015-07-29 12:14:58,966 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0007	CONTAINERID=container_1438157687835_0007_01_000003
2015-07-29 12:14:59,013 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0007_01_000003 transitioned from LOCALIZED to RUNNING
2015-07-29 12:14:59,020 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0007/container_1438157687835_0007_01_000003/default_container_executor.sh]
2015-07-29 12:14:59,730 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1438157687835_0007_01_000002]
2015-07-29 12:15:01,663 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1438157687835_0007_01_000003
2015-07-29 12:15:01,696 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25541 for container-id container_1438157687835_0007_01_000003: 80.7 MB of 1 GB physical memory used; 724.6 MB of 2.1 GB virtual memory used
2015-07-29 12:15:01,751 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 149.9 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 12:15:04,802 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25541 for container-id container_1438157687835_0007_01_000003: 111.2 MB of 1 GB physical memory used; 757.9 MB of 2.1 GB virtual memory used
2015-07-29 12:15:04,849 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 149.9 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 12:15:07,886 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25541 for container-id container_1438157687835_0007_01_000003: 135.1 MB of 1 GB physical memory used; 759.0 MB of 2.1 GB virtual memory used
2015-07-29 12:15:07,921 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 149.9 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 12:15:08,848 INFO org.apache.hadoop.mapred.ShuffleHandler: Setting connection close header...
2015-07-29 12:15:10,966 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25541 for container-id container_1438157687835_0007_01_000003: 142.1 MB of 1 GB physical memory used; 770.1 MB of 2.1 GB virtual memory used
2015-07-29 12:15:11,004 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 149.9 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 12:15:13,364 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0007_000001 (auth:SIMPLE)
2015-07-29 12:15:13,374 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Stopping container with container Id: container_1438157687835_0007_01_000003
2015-07-29 12:15:13,374 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0007	CONTAINERID=container_1438157687835_0007_01_000003
2015-07-29 12:15:13,375 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0007_01_000003 transitioned from RUNNING to KILLING
2015-07-29 12:15:13,375 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1438157687835_0007_01_000003
2015-07-29 12:15:13,430 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1438157687835_0007_01_000003 is : 143
2015-07-29 12:15:13,533 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0007_01_000003 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
2015-07-29 12:15:13,534 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0007/container_1438157687835_0007_01_000003
2015-07-29 12:15:13,541 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1438157687835_0007	CONTAINERID=container_1438157687835_0007_01_000003
2015-07-29 12:15:13,541 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0007_01_000003 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
2015-07-29 12:15:13,542 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1438157687835_0007_01_000003 from application application_1438157687835_0007
2015-07-29 12:15:13,542 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1438157687835_0007
2015-07-29 12:15:14,004 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1438157687835_0007_01_000003
2015-07-29 12:15:14,026 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 150.7 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 12:15:17,071 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 151.4 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 12:15:20,121 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 25380 for container-id container_1438157687835_0007_01_000001: 151.4 MB of 2 GB physical memory used; 1.6 GB of 4.2 GB virtual memory used
2015-07-29 12:15:20,194 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Container container_1438157687835_0007_01_000001 succeeded 
2015-07-29 12:15:20,194 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0007_01_000001 transitioned from RUNNING to EXITED_WITH_SUCCESS
2015-07-29 12:15:20,194 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1438157687835_0007_01_000001
2015-07-29 12:15:20,264 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0007/container_1438157687835_0007_01_000001
2015-07-29 12:15:20,271 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1438157687835_0007	CONTAINERID=container_1438157687835_0007_01_000001
2015-07-29 12:15:20,271 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1438157687835_0007_01_000001 transitioned from EXITED_WITH_SUCCESS to DONE
2015-07-29 12:15:20,271 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1438157687835_0007_01_000001 from application application_1438157687835_0007
2015-07-29 12:15:20,271 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1438157687835_0007
2015-07-29 12:15:20,437 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1438157687835_0007_000001 (auth:SIMPLE)
2015-07-29 12:15:20,456 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Stopping container with container Id: container_1438157687835_0007_01_000001
2015-07-29 12:15:20,456 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=nadir	IP=127.0.0.1	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1438157687835_0007	CONTAINERID=container_1438157687835_0007_01_000001
2015-07-29 12:15:20,459 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1438157687835_0007_01_000001]
2015-07-29 12:15:20,459 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0007 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP
2015-07-29 12:15:20,460 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-nadir/nm-local-dir/usercache/nadir/appcache/application_1438157687835_0007
2015-07-29 12:15:20,471 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_STOP for appId application_1438157687835_0007
2015-07-29 12:15:20,471 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1438157687835_0007 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED
2015-07-29 12:15:20,471 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler: Scheduling Log Deletion for application: application_1438157687835_0007, with delay of 10800 seconds
2015-07-29 12:15:23,121 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1438157687835_0007_01_000001
2015-07-29 14:42:31,471 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting path : /usr/local/hadoop/logs/userlogs/application_1438157687835_0001
2015-07-29 14:51:51,553 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting path : /usr/local/hadoop/logs/userlogs/application_1438157687835_0003
2015-07-29 14:56:41,873 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting path : /usr/local/hadoop/logs/userlogs/application_1438157687835_0004
2015-07-29 15:20:38,565 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting path : /usr/local/hadoop/logs/userlogs/application_1438157687835_0005
2015-07-29 15:42:15,419 ERROR org.apache.hadoop.yarn.server.nodemanager.NodeManager: RECEIVED SIGNAL 15: SIGTERM
2015-07-29 15:42:15,478 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8042
2015-07-29 15:42:15,493 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Applications still running : [application_1438157687835_0006, application_1438157687835_0007]
2015-07-29 15:42:15,493 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Waiting for Applications to be Finished
2015-07-29 15:42:19,494 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Done waiting for Applications to be Finished. Still alive: [application_1438157687835_0006, application_1438157687835_0007]
2015-07-29 15:42:19,494 INFO org.apache.hadoop.ipc.Server: Stopping server on 60035
2015-07-29 15:42:19,497 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2015-07-29 15:42:19,497 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 60035
2015-07-29 15:43:35,386 INFO org.apache.hadoop.yarn.server.nodemanager.NodeManager: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NodeManager
STARTUP_MSG:   host = morfus/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/etc/hadoop/nm-config/log4j.properties
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.7.0_79
************************************************************/
2015-07-29 15:43:35,460 INFO org.apache.hadoop.yarn.server.nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]
2015-07-29 15:43:43,368 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher
2015-07-29 15:43:43,396 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher
2015-07-29 15:43:43,397 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService
2015-07-29 15:43:43,398 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices
2015-07-29 15:43:43,435 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl
2015-07-29 15:43:43,436 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher
2015-07-29 15:43:43,709 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl
2015-07-29 15:43:43,710 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager
2015-07-29 15:43:44,038 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-07-29 15:43:44,589 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-07-29 15:43:44,589 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NodeManager metrics system started
2015-07-29 15:43:44,789 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler
2015-07-29 15:43:44,796 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService
2015-07-29 15:43:44,796 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: per directory file limit = 8192
2015-07-29 15:43:45,068 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: usercache path : file:/tmp/hadoop-nadir/nm-local-dir/usercache_DEL_1438177424821
2015-07-29 15:43:45,095 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting path : file:/tmp/hadoop-nadir/nm-local-dir/usercache_DEL_1438177424821/nadir
2015-07-29 15:43:45,543 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker
2015-07-29 15:43:45,876 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: The Auxilurary Service named 'mapreduce_shuffle' in the configuration is for class org.apache.hadoop.mapred.ShuffleHandler which has a name of 'httpshuffle'. Because these are not the same tools trying to send ServiceData and read Service Meta Data may have issues unless the refer to the name in the config.
2015-07-29 15:43:45,876 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Adding auxiliary service httpshuffle, "mapreduce_shuffle"
2015-07-29 15:43:46,178 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin@28330ce6
2015-07-29 15:43:46,178 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null
2015-07-29 15:43:46,178 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Physical memory check enabled: true
2015-07-29 15:43:46,178 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Virtual memory check enabled: true
2015-07-29 15:43:46,211 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: NodeManager configured with 8 G physical memory allocated to containers, which is more than 80% of the total physical memory available (5.7 G). Thrashing might happen.
2015-07-29 15:43:46,236 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager for null: physical-memory=8192 virtual-memory=17204 virtual-cores=8
2015-07-29 15:43:46,397 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-07-29 15:43:46,475 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 49307
2015-07-29 15:43:46,689 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server
2015-07-29 15:43:46,689 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Blocking new container-requests as container manager rpc server is still starting.
2015-07-29 15:43:46,689 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-07-29 15:43:46,713 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 49307: starting
2015-07-29 15:43:46,736 INFO org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager: Updating node address : morfus:49307
2015-07-29 15:43:46,787 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-07-29 15:43:46,788 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8040
2015-07-29 15:43:46,821 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server
2015-07-29 15:43:46,822 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-07-29 15:43:46,835 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Localizer started on port 8040
2015-07-29 15:43:46,843 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8040: starting
2015-07-29 15:43:46,892 INFO org.apache.hadoop.mapred.IndexCache: IndexCache created with max memory = 10485760
2015-07-29 15:43:46,946 INFO org.apache.hadoop.mapred.ShuffleHandler: httpshuffle listening on port 13562
2015-07-29 15:43:46,983 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: ContainerManager started at morfus/127.0.1.1:49307
2015-07-29 15:43:46,983 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: ContainerManager bound to 0.0.0.0/0.0.0.0:0
2015-07-29 15:43:46,985 INFO org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer: Instantiating NMWebApp at 0.0.0.0:8042
2015-07-29 15:43:47,306 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-07-29 15:43:47,363 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-07-29 15:43:47,380 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined
2015-07-29 15:43:47,404 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-07-29 15:43:47,414 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node
2015-07-29 15:43:47,424 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-07-29 15:43:47,424 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-07-29 15:43:47,441 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /node/*
2015-07-29 15:43:47,442 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /ws/*
2015-07-29 15:43:47,508 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 8042
2015-07-29 15:43:47,508 INFO org.mortbay.log: jetty-6.1.26
2015-07-29 15:43:47,694 INFO org.mortbay.log: Extract jar:file:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar!/webapps/node to /tmp/Jetty_0_0_0_0_8042_node____19tj0x/webapp
2015-07-29 15:43:49,226 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8042
2015-07-29 15:43:49,227 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app /node started at 8042
2015-07-29 15:43:50,624 INFO org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules
2015-07-29 15:43:50,662 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8031
2015-07-29 15:43:50,726 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []
2015-07-29 15:43:50,736 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]
2015-07-29 15:43:51,219 INFO org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id 26299130
2015-07-29 15:43:51,224 INFO org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 1334005260
2015-07-29 15:43:51,225 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as morfus:49307 with total resource of <memory:8192, vCores:8>
2015-07-29 15:43:51,225 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Notifying ContainerManager to unblock new container-requests
